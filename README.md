# micro-stress Boilerplate

This project provides a (hopefully) less stressful boilerplate for building Node.js microservices using:

*   **Fastify:** High-performance web framework.
*   **Knex.js:** SQL query builder for PostgreSQL (and other databases).
*   **OpenTelemetry:** Instrumentation for distributed tracing and metrics.
*   **PostgreSQL:** Relational database.

## Features

*   Modular structure (config, core, modules, database, bootstrap).
*   Centralized configuration using environment variables (`.env`), validated by **Zod** on startup.
*   Knex setup for query building.
*   Database migrations managed by Knex CLI (run manually).
*   Database seeding managed by Knex CLI (run manually).
*   **Lightweight Event-Driven Architecture via Redis Pub/Sub** (abstracted in `core/event-bus.js`).
*   OpenTelemetry tracing configured (HTTP, Knex, Fastify, **Redis**) with OTLP gRPC exporter.
*   Basic Fastify server setup with **structured logging (Pino)** automatically including **OpenTelemetry trace context** and global error handling.
*   Example module (`contacts`) using a functional approach and **publishing events**.
*   Example listener module (`audit-log`) **subscribing to events**.
*   Basic health check endpoint (`/health`).
*   Docker support (`docker-compose.yml` for DB, OTel Collector, **Redis**).

## Prerequisites

*   Node.js (v20.x or later recommended - see `package.json` engines)
*   npm or yarn
*   Docker and Docker Compose (for running dependencies like Postgres and OTel Collector)

## Setup

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/heyjunin/micro-stress
    cd micro-stress
    ```

2.  **Install dependencies:**
    ```bash
    npm install
    ```

3.  **Configure environment variables:**
    *   Copy the example environment file:
        ```bash
        cp .env.example .env
        ```
    *   Edit the `.env` file and set your database credentials, **Redis URL**, OTel endpoint, etc. Variables are validated on startup.
    *   You can also set `SEED_CONTACT_COUNT` to control how many fake contacts are generated by `npm run db:seed:run` (default is 25 if not set).

4.  **Start dependencies (Database, OTel Collector):**
    ```bash
    docker-compose up -d
    ```
    *(Ensure the services in `docker-compose.yml` match your `.env` configuration, especially ports and credentials)*

5.  **Run database migrations:**
    ```bash
    npm run db:migrate:latest
    ```

6.  **(Optional) Run database seeds:**
    ```bash
    npm run db:seed:run
    ```

## Event-Driven Architecture (EDA)

This boilerplate includes a basic EDA implementation using Redis Pub/Sub for asynchronous communication and decoupling.

*   **Broker:** Redis (managed via Docker Compose).
*   **Abstraction:** The `core/event-bus.js` module provides simple `publish(eventName, payload)` and `subscribe(eventName, handler)` functions, hiding the Redis specifics.
*   **Publishing:** Services (like `contacts.service.js`) publish events (e.g., `contact.created`) after completing core operations.
*   **Subscribing:** Listener modules (like `modules/audit-log/listeners.js`) subscribe to specific events and execute handlers when those events are received.
*   **Observability:** Redis commands (publish/subscribe) are traced via OpenTelemetry.

To add new event-driven behavior:
1.  **Define Event:** Decide on an event name (e.g., `order.placed`).
2.  **Publish:** In the service where the action occurs, import `publish` from `core/event-bus.js` and call it with the event name and payload after the action succeeds.
3.  **Create Listener Module:** Create a new module (e.g., `modules/notifications/listeners.js`).
4.  **Subscribe:** In the listener module, import `subscribe` from `core/event-bus.js` and register your handler function for the event name.
5.  **Initialize Listener:** Import and call the listener initialization function (e.g., `initializeNotificationListeners()`) in `app.js` alongside `initializeAuditListeners()`.

This setup allows modules to react to events in other parts of the system without direct coupling.

## Running the Application

*   **Development mode (with hot-reloading and pretty logs):**
    ```bash
    npm run dev
    ```
*   **Production mode:**
    ```bash
    npm start
    ```

The server will start on the port specified in your `.env` file (default: 8080).

## API Documentation (Swagger)

Once the application is running, you can access the interactive API documentation (Swagger UI) at:

[http://localhost:PORT/documentation](http://localhost:PORT/documentation)

Replace `PORT` with the actual port number your application is running on (e.g., 8080).

## Available Scripts

*   `npm start`: Starts the application in production mode.
*   `npm run dev`: Starts the application in development mode with file watching.
*   `npm run lint`: (Placeholder) Run linters.
*   `npm run test`: (Placeholder) Run tests.
*   `npm run db:migrate:make <migration_name>`: Creates a new migration file.
*   `npm run db:migrate:latest`: Runs all pending migrations.
*   `npm run db:migrate:rollback`: Rolls back the last batch of migrations.
*   `npm run db:migrate:fresh`: **[DANGER]** Rolls back *all* migrations and runs them again (resets DB schema).
*   `npm run db:migrate:fresh:seed`: **[DANGER]** Runs `db:migrate:fresh` and then runs seeds.
*   `npm run db:seed:make <seed_name>`: Creates a new seed file.
*   `npm run db:seed:run`: Runs all seed files.
*   `npm run cli:status`: Displays current configuration status.
*   `npm run cli:check-db`: Checks the database connection.
*   `npm run cli:migrations:status`: Shows the status of executed/pending migrations.
*   `npm run docker:up`: Starts all services defined in `docker-compose.yml` in the background.
*   `npm run docker:down`: Stops and removes containers, networks defined in `docker-compose.yml`.
*   `npm run docker:stop`: Stops running containers without removing them.
*   `npm run docker:prune`: **[DANGER]** Stops/removes containers and **removes named volumes** (database data will be lost).
*   `npm run docker:build`: Builds or rebuilds the images for services.
*   `npm run docker:build:nc`: Rebuilds images without using Docker's cache.
*   `npm run docker:ps`: Lists running Docker Compose containers.
*   `npm run docker:logs`: Shows and follows logs from all running Docker Compose services.

## Logging

The application uses Pino for structured logging. In development (`NODE_ENV=development`), logs are prettified using `pino-pretty`.

Crucially, the logger is configured to automatically inject OpenTelemetry `trace_id` and `span_id` into log records when running within a traced request context. This allows easy correlation between logs and traces in observability platforms.

Use `request.log` within your route handlers (controllers) to automatically include request context in your logs:

```javascript
// Example in a controller
async someRequestHandler(request, reply) {
  request.log.info({ contactId: id }, 'Fetching contact');
  try {
    // ... logic ...
    request.log.debug('Contact data processed');
    return reply.send(data);
  } catch (error) {
    request.log.error({ err: error }, 'Failed to fetch contact');
    throw error;
  }
}
```

Use `app.log` for logs outside the request lifecycle (e.g., in bootstrap files).

## Next Steps / TODO

*   Implement comprehensive testing (unit, integration, E2E).
*   Configure linters and formatters (ESLint, Prettier).
*   Enhance OpenTelemetry: Add **Metrics**, more detailed **Resource detectors**, **Sampling** strategies.
*   Refine **Dockerfile** for production builds (multi-stage builds, non-root user).
*   Add authentication/authorization.
*   Implement more modules/features based on this boilerplate.
*   Add query parameter validation and handling (pagination, sorting, filtering) to list endpoints.
*   Improve error handling granularity (custom error classes).
